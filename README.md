# üå§Ô∏è Pipeline M√©t√©o ‚Äì Projet ETL

Ce projet met en place un pipeline automatis√© qui r√©cup√®re quotidiennement des donn√©es m√©t√©orologiques pour plusieurs villes fran√ßaises, les nettoie, puis les stocke dans une base **PostgreSQL**. Le pipeline est orchestr√© via **Apache Airflow**.

###  Objectifs
Fournir des donn√©es m√©t√©o fiables pour :
- Anticiper les retards logistiques li√©s aux conditions climatiques
- Alimenter les analyses des data scientists/analystes m√©tier


## Technologies utilis√©es

- **Python** : d√©veloppement et orchestration
- **Pandas** : nettoyage et transformation des donn√©es
- **PostgreSQL** : stockage des donn√©es transform√©es
- **Airflow** : planification et orchestration ETL
- **API Open-Meteo** : source de donn√©es m√©t√©o
- **SQL** : gestion des tables et insertion


## Lancer le projet

1. Cloner le d√©p√¥t :  
   `git clone <url_du_repo>`

2. Cr√©er un environnement virtuel :  
   `python -m venv venv && source venv/Scripts/activate` (Windows)  
   ou `source venv/bin/activate` (Linux/Mac)

3. Installer les d√©pendances :  
   `pip install -r requirements.txt`

4. Configurer votre base PostgreSQL et ex√©cuter le script dans `db/` pour cr√©er la table.

5. Lancer Airflow (scheduler & webserver) et ex√©cuter le DAG.


## Commandes utiles

```bash
# Activer l'environnement virtuel
source venv/Scripts/activate  # Windows
source venv/bin/activate      # Unix

# Installer les d√©pendances
pip install -r requirements.txt